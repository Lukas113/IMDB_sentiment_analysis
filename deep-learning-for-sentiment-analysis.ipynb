{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Sentiment Analysis\n",
    "\n",
    "This Jupyter Notebook illustrates the sentiment analysis of the IMDB dataset of the Kaggle competition https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews of over 50'000 movie reviews using deep learning. In order to run the notebook, the used packages like `troch` or `tensorflow` have to be installed first. Furthermore the word-embeddings have to be downloaded and saved into the appropriate folders. A more specific explanation of this can be found in the respective chapters.\n",
    "\n",
    "The founding author of this notebook is Tien Tran https://www.kaggle.com/tientd95, who has published his work on https://www.kaggle.com/tientd95/deep-learning-for-sentiment-analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"0.1\"></a>\n",
    "\n",
    "# **Table of Contents**\n",
    "\n",
    "\n",
    "1.\t[Processing Dataset](#1)\n",
    "2.  [Pretrained Word Embedding](#2)\n",
    "3.  [Building Model Pipeline](#3)\n",
    "4.  [Training Model with fastText Embedding](#4)\n",
    "5.  [Training Model with GloVe Embedding](#5)\n",
    "6.  [Model Evaluation](#6)\n",
    "7.  [Interact with User's Input Review](#7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "import os, re, csv, math, codecs\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf  # we use both tensorflow and pytorch (pytorch for main part) , tensorflow for tokenizer\n",
    "\n",
    "from  utils import train_test_split, evaluate\n",
    "\n",
    "torch.manual_seed(42);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Processing Dataset** <a class=\"anchor\" id=\"1\"></a>\n",
    "\n",
    "The reviews are stored in the approx. 66.21 MB large 'IMDB Dataset.csv' in the directory 'data'. These have to be included first and can be downloaded from https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/IMDB Dataset.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>First thing I noticed in this movie of course,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I began watching this movie on t.v. some weeks...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The story was well plotted and interesting by ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  First thing I noticed in this movie of course,...          0\n",
       "1  I began watching this movie on t.v. some weeks...          0\n",
       "2  The story was well plotted and interesting by ...          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert sentiment columns to numerical values\n",
    "df.sentiment = df.sentiment.apply(lambda x: 1 if x=='positive' else 0)\n",
    "\n",
    "X_train, X_test = train_test_split(df)\n",
    "\n",
    "# Random the rows of data\n",
    "X_train = X_train.sample(frac=1).reset_index(drop=True)\n",
    "# get label\n",
    "y_train = X_train.sentiment.values\n",
    "\n",
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check the distribution of the sentiments in all sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([20007, 19993]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(X_train.sentiment.values, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([4993, 5007]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(X_test.sentiment.values, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Pretrained Word Embedding** <a class=\"anchor\" id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fastText** is a word embedding development by Facebook released in 2016.\n",
    "fastText improves on Word2Vec by taking word parts into account, enables training of embeddings on smaller datasets and generalization to unknown words.\n",
    "\n",
    "The full version of fastText can be found here : https://fasttext.cc/docs/en/english-vectors.html \n",
    "\n",
    "Due to the size of memory, ( the full version is about 13GB RAM after loading), we use the mini version of fastText. This reduced embedding can be downloaded from https://fasttext.cc/docs/en/pretrained-vectors.html 'Simple English: bin+text, text' or directly from https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.simple.zip.\n",
    "\n",
    "The embeddings are all stored in the 'embeddings' directory. In this folder there is a subfolder for each embedding, if they contain more than one file. fastText embedding for example should be found under './embeddings/wiki.simple/wiki.simple.vec'.\n",
    "\n",
    "The vectors of this embedding have a length of 300. In the file these are organized in such a way that vectors of a length of 301 are stored. The first value in each case is the word in the vocabulary, whereas the 300 next elements represent the embedding vectors. Thus we extract the embedding into a dictionary `fasttext_embedding` with the key as the word and the value as the embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111052it [00:07, 14039.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load fasttext embeddings\n",
    "print('loading word embeddings...')\n",
    "fasttext_embedding = {}\n",
    "f = codecs.open('./embeddings/wiki.simple/wiki.simple.vec', encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    fasttext_embedding[word] = coefs\n",
    "f.close()\n",
    "\n",
    "# Check the dimensions of fastText\n",
    "fasttext_embedding['hello'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GloVe** (Global Vectors) is a word embedding which is developed as an open-source project at Stanford an was launched in 2014. The model is trained in an unsupervised manner for obtaining vector representations for words. It is based on co-occurence statistics from a corpus to map them into a semantical meaningful subspace.\n",
    "\n",
    "GloVe can be downloaded from https://www.kaggle.com/danielwillgeorge/glove6b100dtxt 'glove.6B.100d.txt'.\n",
    "\n",
    "The vectors of this embedding have a length of 100 and can be extracted very easly using dict-comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load GloVe embedding.\n",
    "glove = pd.read_csv('./embeddings/glove/glove.6B.100d.txt', sep=\" \", quoting=3, header=None, index_col=0)\n",
    "glove_embedding = {key: val.values for key, val in glove.T.items()}\n",
    "\n",
    "# Check the dimensions of GloVe\n",
    "glove_embedding['hello'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Building Model Pipeline** <a class=\"anchor\" id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class\n",
    "\n",
    "First we need to create a Dataset class, take input in numpy array(embedding matrix) and return torch tensor output datatype "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset:\n",
    "    def __init__(self, reviews, targets):\n",
    "        \"\"\"\n",
    "        Argument:\n",
    "        reviews: a numpy array\n",
    "        targets: a vector array\n",
    "        \n",
    "        Return xtrain and ylabel in torch tensor datatype, stored in dictionary format\n",
    "        \"\"\"\n",
    "        self.reviews = reviews\n",
    "        self.target = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        # return length of dataset\n",
    "        return len(self.reviews)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # given an idex (item), return review and target of that index in torch tensor\n",
    "        review = torch.tensor(self.reviews[index,:], dtype = torch.long)\n",
    "        target = torch.tensor(self.target[index], dtype = torch.float)\n",
    "        \n",
    "        return {'review': review,\n",
    "                'target': target}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we move on to build a model class. Before that, there's something to remembe**r:\n",
    "\n",
    "* The input feed to model is served as embedding matrix (each row corresponding to an embedding vector of a word)\n",
    "* Number of words (for entire dataset) = number of row in embeddng matrix \n",
    "* Dimension of embedding is the num of columns in matrix, = dimention of pretrained embedding (fasttext, glove,..in case we use pretrained embedding). \n",
    "* Pretrained embeddings have several versions with different dimension so we should check the dimension before set dimension to model.\n",
    "* In case we use pretrainde embedding (this kernel), we will not do gadient calculation on these embedding (required grads = False)\n",
    "* In case we train embedding from scratch, we will treat embedding matrix as weight parameter and training on them (required grads = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embedding_matrix, flatten=False):\n",
    "        \"\"\"\n",
    "        Given embedding_matrix: numpy array with vector for all words\n",
    "        return prediction ( in torch tensor format)\n",
    "        \"\"\"\n",
    "        super(LSTM, self).__init__()\n",
    "        self.flatten = flatten\n",
    "        # Number of words = number of rows in embedding matrix\n",
    "        num_words = embedding_matrix.shape[0]\n",
    "        # Dimension of embedding is num of columns in the matrix\n",
    "        embedding_dim = embedding_matrix.shape[1]\n",
    "        # Define an input embedding layer\n",
    "        self.embedding = nn.Embedding(\n",
    "                                      num_embeddings=num_words,\n",
    "                                      embedding_dim=embedding_dim)\n",
    "        # Embedding matrix actually is collection of parameter\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype = torch.float32))\n",
    "        # Because we use pretrained embedding (GLove, Fastext,etc) so we turn off requires_grad-meaning we do not train gradient on embedding weight\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        # LSTM with hidden_size = 128\n",
    "        self.lstm = nn.LSTM(\n",
    "                            embedding_dim, \n",
    "                            128,\n",
    "                            bidirectional=True,\n",
    "                            batch_first=True,\n",
    "                             )\n",
    "        if flatten:\n",
    "            self.out = nn.Linear(128*256, 1)\n",
    "        else:\n",
    "            # Input(512) because we use bi-directional LSTM ==> hidden_size*2 + maxpooling **2  = 128*4 = 512, will be explained more on forward method\n",
    "            self.out = nn.Linear(512, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # pass input (tokens) through embedding layer\n",
    "        x = self.embedding(x)\n",
    "        # fit embedding to LSTM\n",
    "        hidden, _ = self.lstm(x)\n",
    "        if self.flatten:\n",
    "            flattened = torch.flatten(hidden, start_dim=1)\n",
    "            out = self.out(flattened)\n",
    "        else:\n",
    "            # apply mean and max pooling on lstm output\n",
    "            avg_pool= torch.mean(hidden, 1)\n",
    "            max_pool, index_max_pool = torch.max(hidden, 1)\n",
    "            # concat avg_pool and max_pool ( so we have 256 size, also because this is bidirectional ==> 256*2 = 512)\n",
    "            out = torch.cat((avg_pool, max_pool), 1)\n",
    "            # fit out to self.out to conduct dimensionality reduction to 1\n",
    "            out = self.out(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input of a forward pass is given by a matrix $X \\in \\mathbb{R}^{m \\times n}$. $m$ is the review dimension and $n$ is the word dimension. $X$ contains the indexes of the word embedding given by the word of the reviews. The reviews are defined into a fixed length using sequence padding, so that longer reviews are only considered up to `MAX_LEN` and shorter ones are padded with a special token. Thus, batching is greatly simplified.`torch.LSTM` then detects the padding automatically. Since `bidirectional` is True, the sequences are also passed through the LSTM from behind. So for a review we have a LSTM series of $2 \\times $ `MAX_LEN`. Afterwards the hidden state should contain all the information to perform the sentiment analysis. Because the hidden state is a matrix for a single sample, we need to reduce it's dimension. For this purpose we pursue two attempts:\n",
    "\n",
    "- First attempt is done by average, and max-pooling the hidden state of the LSTM. Afterwards we have left a vector representation of the sentiment analysis, which will be reduced with a linear layer to one dimension for the sentiment analysis. This was the attempt of the original author of this Notebook.\n",
    "- The second attempt is to flatten the output of the hidden state in order to feed it to a wider linear layer.\n",
    "\n",
    "The last linear layer in both versions have a single neuron as output. We then simply can take a threshold of 0.5 to split between a positive or negative review.\n",
    "\n",
    "Now after buidling the model class, we move to create `train` and `predict` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader, model, optimizer, device):\n",
    "    \"\"\"\n",
    "    this is model training for one epoch\n",
    "    data_loader:  this is torch dataloader, just like dataset but in torch and devide into batches\n",
    "    model : lstm\n",
    "    optimizer : torch optimizer : adam\n",
    "    device:  cuda or cpu\n",
    "    \"\"\"\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "    # go through batches of data in data loader\n",
    "    for data in data_loader:\n",
    "        reviews = data['review']\n",
    "        targets = data['target']\n",
    "        # move the data to device that we want to use\n",
    "        reviews = reviews.to(device, dtype = torch.long)\n",
    "        targets = targets.to(device, dtype = torch.float)\n",
    "        # clear the gradient\n",
    "        optimizer.zero_grad()\n",
    "        # make prediction from model\n",
    "        predictions = model(reviews)\n",
    "        # caculate the losses\n",
    "        loss = nn.BCEWithLogitsLoss()(predictions, targets.view(-1,1))\n",
    "        # backprob\n",
    "        loss.backward()\n",
    "        #single optimization step\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_loader, model, device):\n",
    "    final_predictions = []\n",
    "    final_targets = []\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            reviews = data['review']\n",
    "            targets = data['target']\n",
    "            reviews = reviews.to(device, dtype = torch.long)\n",
    "            targets = targets.to(device, dtype=torch.float)\n",
    "            # make prediction\n",
    "            predictions = model(reviews)\n",
    "            # move prediction and target to cpu\n",
    "            predictions = predictions.cpu().numpy().tolist()\n",
    "            targets = data['target'].cpu().numpy().tolist()\n",
    "            # add predictions to final_prediction\n",
    "            final_predictions.extend(predictions)\n",
    "            final_targets.extend(targets)\n",
    "    return final_predictions, final_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 8\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(word_index, embedding_dict=None, d_model=100):\n",
    "    \"\"\"\n",
    "     this function create the embedding matrix save in numpy array\n",
    "    :param word_index: a dictionary with word: index_value\n",
    "    :param embedding_dict: a dict with word embedding\n",
    "    :d_model: the dimension of word pretrained embedding, here I just set to 100, we will define again\n",
    "    :return a numpy array with embedding vectors for all known words\n",
    "    \"\"\"\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, d_model))\n",
    "    ## loop over all the words\n",
    "    for word, index in word_index.items():\n",
    "        if word in embedding_dict:\n",
    "            embedding_matrix[index] = embedding_dict[word]\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is time to running the model. \n",
    "The entire workflow will be as following steps:\n",
    "\n",
    "==> **Step1**: Creating a tokenizer function to convert sentences of dataset to token index\n",
    "After converting we'll have a dictionary contain word and its index. We feed it to creating an embedding matrix\n",
    "\n",
    "==> **Step2**: Cross validation of dataset to devide into train_df and valid_df\n",
    "\n",
    "==> **Step3**: Applying tokenizer pad_sequence to token index to ensure all sentence has the same vector dimension ( example: the sentence with 10 words will have longer vector dimension then the sentence with 2 words, using pad_sequence to ensure the same length, The length is set to a fixed number)\n",
    "\n",
    "We used the tokenizer from `tensorflow` 2 because of its convenience in pad_sequence.\n",
    "\n",
    "==> **Step4**: Initialize dataset class `IMDBDataset`\n",
    "\n",
    "==> **Step5**: We load the dataset which created in step4 to Pytorch DataLoader in order to devide the dataset to batches\n",
    "\n",
    "==> **Step6**: Till now, we have almost necessary components to start training. Calling model, optimizer, send model to device and start running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Training Model with fastText Embedding** <a class=\"anchor\" id=\"4\"></a>\n",
    "\n",
    "\n",
    "fastText embedding version in this kernel is 300, so we set d_model =300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Tokenization \n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(df.review.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load fasttext embedding\n",
      "training model\n",
      "epoch: 0, accuracy_score: 0.8529\n",
      "epoch: 1, accuracy_score: 0.8626\n",
      "epoch: 2, accuracy_score: 0.8684\n",
      "epoch: 3, accuracy_score: 0.8705\n",
      "epoch: 4, accuracy_score: 0.873\n"
     ]
    }
   ],
   "source": [
    "print('Load fasttext embedding')\n",
    "embedding_matrix = create_embedding_matrix(tokenizer.word_index, embedding_dict=fasttext_embedding, d_model=300)\n",
    "\n",
    "# pad sequence\n",
    "xtrain = tokenizer.texts_to_sequences(X_train.review.values)\n",
    "xtest = tokenizer.texts_to_sequences(X_test.review.values)\n",
    "\n",
    "# zero padding\n",
    "xtrain = tf.keras.preprocessing.sequence.pad_sequences(xtrain, maxlen=MAX_LEN)\n",
    "xtest = tf.keras.preprocessing.sequence.pad_sequences(xtest, maxlen=MAX_LEN)\n",
    "\n",
    "# initialize dataset class for training\n",
    "train_dataset = IMDBDataset(reviews=xtrain, targets=X_train.sentiment.values)\n",
    "\n",
    "# Load dataset to Pytorch DataLoader\n",
    "# after we have train_dataset, we create a torch dataloader to load train_dataset class based on specified batch_size\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size = TRAIN_BATCH_SIZE, num_workers=2)\n",
    "# initialize dataset class for validation\n",
    "valid_dataset = IMDBDataset(reviews=xtest, targets=X_test.sentiment.values)\n",
    "valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = VALID_BATCH_SIZE, num_workers=1)\n",
    "\n",
    "# Running \n",
    "device = torch.device('cuda')\n",
    "# feed embedding matrix to lstm\n",
    "model_fasttext = LSTM(embedding_matrix, flatten=False)\n",
    "# set model to cuda device\n",
    "model_fasttext.to(device)\n",
    "# initialize Adam optimizer\n",
    "optimizer = torch.optim.Adam(model_fasttext.parameters(), lr=1e-3)\n",
    "\n",
    "print('training model')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    #train one epoch\n",
    "    train(train_data_loader, model_fasttext, optimizer, device)\n",
    "    #validate\n",
    "    outputs_fasttext, targets_fasttext = predict(valid_data_loader, model_fasttext, device)\n",
    "    # threshold\n",
    "    outputs_fasttext = np.array(outputs_fasttext) >= 0.5\n",
    "    # calculate accuracy\n",
    "    accuracy = metrics.accuracy_score(targets_fasttext, outputs_fasttext)\n",
    "    print(f'epoch: {epoch}, accuracy_score: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we flatten the output of the hidden state instead of pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load fasttext embedding\n",
      "training model flatten\n",
      "epoch: 0, accuracy_score: 0.8307\n",
      "epoch: 1, accuracy_score: 0.8395\n",
      "epoch: 2, accuracy_score: 0.8169\n",
      "epoch: 3, accuracy_score: 0.8125\n",
      "epoch: 4, accuracy_score: 0.8149\n"
     ]
    }
   ],
   "source": [
    "print('Load fasttext embedding')\n",
    "embedding_matrix = create_embedding_matrix(tokenizer.word_index, embedding_dict=fasttext_embedding, d_model=300)\n",
    "\n",
    "# pad sequence\n",
    "xtrain = tokenizer.texts_to_sequences(X_train.review.values)\n",
    "xtest = tokenizer.texts_to_sequences(X_test.review.values)\n",
    "\n",
    "# zero padding\n",
    "xtrain = tf.keras.preprocessing.sequence.pad_sequences(xtrain, maxlen=MAX_LEN)\n",
    "xtest = tf.keras.preprocessing.sequence.pad_sequences(xtest, maxlen=MAX_LEN)\n",
    "\n",
    "# initialize dataset class for training\n",
    "train_dataset = IMDBDataset(reviews=xtrain, targets=X_train.sentiment.values)\n",
    "\n",
    "# Load dataset to Pytorch DataLoader\n",
    "# after we have train_dataset, we create a torch dataloader to load train_dataset class based on specified batch_size\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size = TRAIN_BATCH_SIZE, num_workers=2)\n",
    "# initialize dataset class for validation\n",
    "valid_dataset = IMDBDataset(reviews=xtest, targets=X_test.sentiment.values)\n",
    "valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = VALID_BATCH_SIZE, num_workers=1)\n",
    "\n",
    "# Running \n",
    "device = torch.device('cuda')\n",
    "# feed embedding matrix to lstm\n",
    "model_fasttext_flatten = LSTM(embedding_matrix, flatten=True)\n",
    "# set model to cuda device\n",
    "model_fasttext_flatten.to(device)\n",
    "# initialize Adam optimizer\n",
    "optimizer = torch.optim.Adam(model_fasttext_flatten.parameters(), lr=1e-3)\n",
    "\n",
    "print('training model flatten')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    #train one epoch\n",
    "    train(train_data_loader, model_fasttext_flatten, optimizer, device)\n",
    "    #validate\n",
    "    outputs_fasttext_flatten, targets_fasttext_flatten = predict(valid_data_loader, model_fasttext_flatten, device)\n",
    "    # threshold\n",
    "    outputs_fasttext_flatten = np.array(outputs_fasttext_flatten) >= 0.5\n",
    "    # calculate accuracy\n",
    "    accuracy = metrics.accuracy_score(targets_fasttext_flatten, outputs_fasttext_flatten)\n",
    "    print(f'epoch: {epoch}, accuracy_score: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Training Model with GloVe Embedding** <a class=\"anchor\" id=\"5\"></a>\n",
    "\n",
    "GloVe embedding version in this kernel is 100, so we set d_model =100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Glove embedding\n",
      "training model\n",
      "epoch: 0, accuracy_score: 0.855\n",
      "epoch: 1, accuracy_score: 0.8677\n",
      "epoch: 2, accuracy_score: 0.8751\n",
      "epoch: 3, accuracy_score: 0.8779\n",
      "epoch: 4, accuracy_score: 0.8721\n"
     ]
    }
   ],
   "source": [
    "print('Load Glove embedding')\n",
    "embedding_matrix = create_embedding_matrix(tokenizer.word_index, embedding_dict=glove_embedding, d_model=100)\n",
    "\n",
    "# pad sequence\n",
    "xtrain = tokenizer.texts_to_sequences(X_train.review.values)\n",
    "xtest = tokenizer.texts_to_sequences(X_test.review.values)\n",
    "\n",
    "# zero padding\n",
    "xtrain = tf.keras.preprocessing.sequence.pad_sequences(xtrain, maxlen=MAX_LEN)\n",
    "xtest = tf.keras.preprocessing.sequence.pad_sequences(xtest, maxlen=MAX_LEN)\n",
    "\n",
    "# initialize dataset class for training\n",
    "train_dataset = IMDBDataset(reviews=xtrain, targets=X_train.sentiment.values)\n",
    "\n",
    "# load dataset to Pytorch DataLoader\n",
    "# after we have train_dataset, we create a torch dataloader to load train_dataset class based on specified batch_size\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size = TRAIN_BATCH_SIZE, num_workers=2)\n",
    "# initialize dataset class for validation\n",
    "valid_dataset = IMDBDataset(reviews=xtest, targets=X_test.sentiment.values)\n",
    "valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = VALID_BATCH_SIZE, num_workers=1)\n",
    "\n",
    "# Running \n",
    "device = torch.device('cuda')\n",
    "# feed embedding matrix to lstm\n",
    "model_glove = LSTM(embedding_matrix, flatten=False)\n",
    "# set model to cuda device\n",
    "model_glove.to(device)\n",
    "# initialize Adam optimizer\n",
    "optimizer = torch.optim.Adam(model_glove.parameters(), lr=1e-3)\n",
    "\n",
    "print('training model')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    #train one epoch\n",
    "    train(train_data_loader, model_glove, optimizer, device)\n",
    "    #validate\n",
    "    outputs_glove, targets_glove = predict(valid_data_loader, model_glove, device)\n",
    "    # threshold\n",
    "    outputs_glove = np.array(outputs_glove) >= 0.5\n",
    "    # calculate accuracy\n",
    "    accuracy = metrics.accuracy_score(targets_glove, outputs_glove)\n",
    "    print(f'epoch: {epoch}, accuracy_score: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we flatten the output of the hidden state instead of pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Glove embedding\n",
      "training model flatten\n",
      "epoch: 0, accuracy_score: 0.8376\n",
      "epoch: 1, accuracy_score: 0.8432\n",
      "epoch: 2, accuracy_score: 0.834\n",
      "epoch: 3, accuracy_score: 0.8327\n",
      "epoch: 4, accuracy_score: 0.8316\n"
     ]
    }
   ],
   "source": [
    "print('Load Glove embedding')\n",
    "embedding_matrix = create_embedding_matrix(tokenizer.word_index, embedding_dict=glove_embedding, d_model=100)\n",
    "\n",
    "# pad sequence\n",
    "xtrain = tokenizer.texts_to_sequences(X_train.review.values)\n",
    "xtest = tokenizer.texts_to_sequences(X_test.review.values)\n",
    "\n",
    "# zero padding\n",
    "xtrain = tf.keras.preprocessing.sequence.pad_sequences(xtrain, maxlen=MAX_LEN)\n",
    "xtest = tf.keras.preprocessing.sequence.pad_sequences(xtest, maxlen=MAX_LEN)\n",
    "\n",
    "# initialize dataset class for training\n",
    "train_dataset = IMDBDataset(reviews=xtrain, targets=X_train.sentiment.values)\n",
    "\n",
    "# load dataset to Pytorch DataLoader\n",
    "# after we have train_dataset, we create a torch dataloader to load train_dataset class based on specified batch_size\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size = TRAIN_BATCH_SIZE, num_workers=2)\n",
    "# initialize dataset class for validation\n",
    "valid_dataset = IMDBDataset(reviews=xtest, targets=X_test.sentiment.values)\n",
    "valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = VALID_BATCH_SIZE, num_workers=1)\n",
    "\n",
    "# Running \n",
    "device = torch.device('cuda')\n",
    "# feed embedding matrix to lstm\n",
    "model_glove_flatten = LSTM(embedding_matrix, flatten=True)\n",
    "# set model to cuda device\n",
    "model_glove_flatten.to(device)\n",
    "# initialize Adam optimizer\n",
    "optimizer = torch.optim.Adam(model_glove_flatten.parameters(), lr=1e-3)\n",
    "\n",
    "print('training model flatten')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    #train one epoch\n",
    "    train(train_data_loader, model_glove_flatten, optimizer, device)\n",
    "    #validate\n",
    "    outputs_glove_flatten, targets_glove_flatten = predict(valid_data_loader, model_glove_flatten, device)\n",
    "    # threshold\n",
    "    outputs_glove_flatten = np.array(outputs_glove_flatten) >= 0.5\n",
    "    # calculate accuracy\n",
    "    accuracy = metrics.accuracy_score(targets_glove_flatten, outputs_glove_flatten)\n",
    "    print(f'epoch: {epoch}, accuracy_score: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6. Model Evaluation** <a class=\"anchor\" id=\"6\"></a>\n",
    "\n",
    "In this section we're looking at the evaluation of our four models. The models where the hidden state was flattened instead of pooled have the ending `_flatten`.\n",
    "\n",
    "## fastText\n",
    "\n",
    "**Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.873\n"
     ]
    }
   ],
   "source": [
    "evaluation_fasttext = evaluate(targets_fasttext, outputs_fasttext)\n",
    "print(evaluation_fasttext[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8149\n"
     ]
    }
   ],
   "source": [
    "evaluation_fasttext_flatten = evaluate(targets_fasttext_flatten, outputs_fasttext_flatten)\n",
    "print(evaluation_fasttext_flatten[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**classification-report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.84      0.92      0.88      4993\n",
      "    Positive       0.91      0.83      0.87      5007\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.88      0.87      0.87     10000\n",
      "weighted avg       0.88      0.87      0.87     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(evaluation_fasttext[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.87      0.74      0.80      4993\n",
      "    Positive       0.77      0.89      0.83      5007\n",
      "\n",
      "    accuracy                           0.81     10000\n",
      "   macro avg       0.82      0.81      0.81     10000\n",
      "weighted avg       0.82      0.81      0.81     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(evaluation_fasttext_flatten[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe\n",
    "\n",
    "**Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8721\n"
     ]
    }
   ],
   "source": [
    "evaluation_glove = evaluate(targets_glove, outputs_glove)\n",
    "print(evaluation_glove[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8316\n"
     ]
    }
   ],
   "source": [
    "evaluation_glove_flatten = evaluate(targets_glove_flatten, outputs_glove_flatten)\n",
    "print(evaluation_glove_flatten[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**classification-report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.83      0.94      0.88      4993\n",
      "    Positive       0.93      0.80      0.86      5007\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.88      0.87      0.87     10000\n",
      "weighted avg       0.88      0.87      0.87     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(evaluation_glove[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.88      0.77      0.82      4993\n",
      "    Positive       0.80      0.89      0.84      5007\n",
      "\n",
      "    accuracy                           0.83     10000\n",
      "   macro avg       0.84      0.83      0.83     10000\n",
      "weighted avg       0.84      0.83      0.83     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(evaluation_glove_flatten[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "We don't really know why the flattened version of the LSTM's hidden state does perform worse than the pooling version. We suspect that the models overfit to training data since the linear layer of the flattened version has much more weights. So we think that the pooling version is better in generalization.\n",
    "\n",
    "Most test have shown that **FastText** and **GloVe** performed pretty similar in accuracy, precision and recall on the test-set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **7. Interact with User's Input Review** <a class=\"anchor\" id=\"7\"></a>\n",
    "\n",
    "Now it's time to test model by entering any review we can think and see the model reaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Interact_user_input(model):\n",
    "    '''\n",
    "    model: trained model : fasttext model or glove model\n",
    "    '''\n",
    "    model.eval()\n",
    "    \n",
    "    sentence = ''\n",
    "    while True:\n",
    "        try:\n",
    "            sentence = input('Review: ')\n",
    "            if sentence in ['q','quit']: \n",
    "                break\n",
    "            sentence = np.array([sentence])\n",
    "            sentence_token = tokenizer.texts_to_sequences(sentence)\n",
    "            sentence_token = tf.keras.preprocessing.sequence.pad_sequences(sentence_token, maxlen = MAX_LEN)\n",
    "            sentence_train = torch.tensor(sentence_token, dtype = torch.long).to(device, dtype = torch.long)\n",
    "            predict = model(sentence_train)\n",
    "            if predict.item() > 0.5:\n",
    "                print('------> Positive')\n",
    "            else:\n",
    "                print('------> Negative')\n",
    "        except KeyError:\n",
    "            print('please enter again')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter reviews and test the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Interact_user_input(model_fasttext)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "npr",
   "language": "python",
   "name": "npr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
