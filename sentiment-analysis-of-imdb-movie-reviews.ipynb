{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "**Sentiment Analysis of IMDB Movie Reviews**\n",
    "\n",
    "\n",
    "This Notebook is based heavily on the Notebook by [Lakshmipathi N](https://www.kaggle.com/lakshmi25npathi) found on [Kaggle](https://www.kaggle.com/lakshmi25npathi/sentiment-analysis-of-imdb-movie-reviews)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1424638f5259100af9f9a5c1b05bd23cf5b71e51"
   },
   "source": [
    "**Import necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#Load the libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# https://online.stat.psu.edu/stat504/lesson/1/1.7\n",
    "from utils import preprocesser_text, binarize_sentiment, train_test_split, evaluate\n",
    "\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "be1b642cce343f7a8f68f8c91f7c50372cdf4381"
   },
   "source": [
    "**Import the training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "4c593c17588723c0b0b0f19851cb70a8447ced76",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing the training data\n",
    "imdb_data=pd.read_csv('data/IMDB Dataset.csv')\n",
    "print(imdb_data.shape)\n",
    "imdb_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1ad3773974351ed9bdf389b2847d7475b36c2295"
   },
   "source": [
    "**Exploratery data analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "7f11c83b1320c8982b36889145f7f770563674a8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>49582</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review sentiment\n",
       "count                                               50000     50000\n",
       "unique                                              49582         2\n",
       "top     Loved today's show!!! It was a variety and not...  positive\n",
       "freq                                                    5     25000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Summary of the dataset\n",
    "imdb_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "453c3fd238f62ab8f649eb01771817e25bc0c77d"
   },
   "source": [
    "**Sentiment count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "cb6bb97b0f851947dcf341a1de5708a1f2bc64c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    25000\n",
       "negative    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentiment count\n",
    "imdb_data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dataset is balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f61964573faababe1f7897b77d32815a24954d2f"
   },
   "source": [
    "**Spliting the training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd2dd2b26814872aa32e9c2ec4a86ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9465fe09ba448cda18d54aff92285f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c807e30e7cf479e8d0fd9e97a3736f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a432cd6f6cfd4b3993367f70d5061702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f008a2eb1bcc414ab7c47dbf53f781a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imdb_data = preprocesser_text(imdb_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "90da29c3b79f46f41d7391a2a116065b616d0fac"
   },
   "source": [
    "**Text normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "b20c242bd091929ca896ea2c6e936ca00efe6ecf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative    20007\n",
      "positive    19993\n",
      "Name: sentiment, dtype: int64\n",
      "positive    5007\n",
      "negative    4993\n",
      "Name: sentiment, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'one review ha mention watch 1 oz episod youll hook right thi exactli happen meth first thing struck oz wa brutal unflinch scene violenc set right word go trust thi show faint heart timid thi show pull punch regard drug sex violenc hardcor classic use wordit call oz nicknam given oswald maximum secur state penitentari focus mainli emerald citi experiment section prison cell glass front face inward privaci high agenda em citi home manyaryan muslim gangsta latino christian italian irish moreso scuffl death stare dodgi deal shadi agreement never far awayi would say main appeal show due fact goe show wouldnt dare forget pretti pictur paint mainstream audienc forget charm forget romanceoz doesnt mess around first episod ever saw struck nasti wa surreal couldnt say wa readi watch develop tast oz got accustom high level graphic violenc violenc injustic crook guard wholl sold nickel inmat wholl kill order get away well manner middl class inmat turn prison bitch due lack street skill prison experi watch oz may becom comfort uncomfort viewingthat get touch darker side'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalized train reviews\n",
    "norm_train, norm_test = train_test_split(imdb_data)\n",
    "print(norm_train.sentiment.value_counts())\n",
    "print(norm_test.sentiment.value_counts())\n",
    "norm_train_reviews=norm_train.review\n",
    "norm_train_reviews[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d69462bb209a66cff86376dc8481d0c0140d894d"
   },
   "source": [
    "**Normalized test reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "c5d0d38bd9976150367e9d75f3b933774c96a1ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hickori dickori dock wa good poirot mysteri confess read book despit avid agatha christi fan adapt isnt without problem time humour valiant attempt get right wa littl overdon event lead final solut rather rush also thought slow moment mysteri felt pad howev love hickori dickori dock wa film veri similar visual style brilliant abc murder realli set atmospher dark camera work dark light darker moment somewhat creepi thi wa help one haunt music score poirot adapt mayb disturb one one two buckl shoe gave nightmar plot complex essenti ingredi though convolut buckl shoeand way good thing act wa veri good david suchet impeccablei know cant use thi word forev cant think better word describ hi perform seri poirot phillip jackson paulin moran justic integr charact brilliantli student great person well develop whole particularli damian lewi leonard solid mysteri doesnt rank along best 7510 bethani cox'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalized test reviews\n",
    "norm_test_reviews=norm_test.review\n",
    "norm_test_reviews.loc[40005]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "52371868f05ff9cf157280c5acf0f5bc71ee176d"
   },
   "source": [
    "**Bags of words model**\n",
    "\n",
    "It is used to convert text documents to numerical vectors or bag of words.\n",
    "\n",
    "> Convert a collection of text documents to a matrix of token counts.\n",
    "> This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.\n",
    "> If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "35cf9dcefb40b2dc520c5b0d559695324c46cc04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW_cv_train: (40000, 6209089)\n",
      "BOW_cv_test: (10000, 6209089)\n"
     ]
    }
   ],
   "source": [
    "#Count vectorizer for bag of words\n",
    "cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\n",
    "#transformed train reviews\n",
    "cv_train_reviews=cv.fit_transform(norm_train_reviews)\n",
    "#transformed test reviews\n",
    "cv_test_reviews=cv.transform(norm_test_reviews)\n",
    "\n",
    "print('BOW_cv_train:',cv_train_reviews.shape)\n",
    "print('BOW_cv_test:',cv_test_reviews.shape)\n",
    "#vocab=cv.get_feature_names()-toget feature names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "52371868f05ff9cf157280c5acf0f5bc71ee176d"
   },
   "source": [
    "**Term Frequency-Inverse Document Frequency model (TFIDF)**\n",
    "\n",
    "It is used to convert text documents to  matrix of  tfidf features.\n",
    "> The formula that is used to compute the tf-idf for a term t of a document d in a document set is tf-idf(t, d) = tf(t, d) * idf(t), and the idf is computed as idf(t) = log [ n / df(t) ] + 1 (if smooth_idf=False), where n is the total number of documents in the document set and df(t) is the document frequency of t; the document frequency is the number of documents in the document set that contain the term t. The effect of adding “1” to the idf in the equation above is that terms with zero idf, i.e., terms that occur in all documents in a training set, will not be entirely ignored. (Note that the idf formula above differs from the standard textbook notation that defines the idf as idf(t) = log [ n / (df(t) + 1) ]).\n",
    "\n",
    "tf(t, d) == # Wort t / max (# Wort t) über alle Dokumente\n",
    "\n",
    "idf(t) == # Dokumente mit Wort t / # Dokumente\n",
    "\n",
    "\n",
    "tf-idf(t, d) = tf(t, d) * idf(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "afe6de957339921e05a6faeaf731f2272fd31946"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf_train: (40000, 6209089)\n",
      "Tfidf_test: (10000, 6209089)\n"
     ]
    }
   ],
   "source": [
    "#Tfidf vectorizer\n",
    "tv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\n",
    "#transformed train reviews\n",
    "tv_train_reviews=tv.fit_transform(norm_train_reviews)\n",
    "#transformed test reviews\n",
    "tv_test_reviews=tv.transform(norm_test_reviews)\n",
    "print('Tfidf_train:',tv_train_reviews.shape)\n",
    "print('Tfidf_test:',tv_test_reviews.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "21a80c94fb42e14391c627710c5d796c40aa7dde"
   },
   "source": [
    "**Split and binarize the sentiment tdata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "ca1e4cc917265ac98a72c37cffe57f27e9897408"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        1\n",
      "1        1\n",
      "2        1\n",
      "3        0\n",
      "4        1\n",
      "        ..\n",
      "39995    1\n",
      "39996    1\n",
      "39997    1\n",
      "39998    0\n",
      "39999    0\n",
      "Name: sentiment, Length: 40000, dtype: int64\n",
      "40000    0\n",
      "40001    0\n",
      "40002    0\n",
      "40003    0\n",
      "40004    0\n",
      "        ..\n",
      "49995    1\n",
      "49996    0\n",
      "49997    0\n",
      "49998    0\n",
      "49999    0\n",
      "Name: sentiment, Length: 10000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Spliting the sentiment data\n",
    "train_sentiments=norm_train.sentiment\n",
    "test_sentiments=norm_test.sentiment\n",
    "\n",
    "test_sentiments = binarize_sentiment(test_sentiments)\n",
    "train_sentiments = binarize_sentiment(train_sentiments)\n",
    "print(train_sentiments)\n",
    "print(test_sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelling the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d5e45fdc9d062a5b9b9dd665ffe732776e196953"
   },
   "source": [
    "Let us build logistic regression model for both bag of words and tfidf features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "142d007421900550079a12ae8655bcae678ebaad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1, max_iter=500, random_state=42)\n",
      "LogisticRegression(C=1, max_iter=500, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "lr=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)\n",
    "#Fitting the model for Bag of words\n",
    "lr_bow=lr.fit(cv_train_reviews,train_sentiments)\n",
    "print(lr_bow)\n",
    "#Fitting the model for tfidf features\n",
    "lr_tfidf=lr.fit(tv_train_reviews,train_sentiments)\n",
    "print(lr_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "07eb6d52eb32469e3be82e90af636d598a7b7c27"
   },
   "source": [
    "**Logistic regression model performane on test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "52ad86935b76117f97b79e6672a3ba12352b9461"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 1 1]\n",
      "[0 0 0 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "#Predicting the model for bag of words\n",
    "lr_bow_predict=lr.predict(cv_test_reviews)\n",
    "print(lr_bow_predict)\n",
    "##Predicting the model for tfidf features\n",
    "lr_tfidf_predict=lr.predict(tv_test_reviews)\n",
    "print(lr_tfidf_predict)\n",
    "\n",
    "#Predicting the model for bag of words\n",
    "lr_bow_predict_train=lr.predict(cv_train_reviews)\n",
    "##Predicting the model for tfidf features\n",
    "lr_tfidf_predict_train=lr.predict(tv_train_reviews)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_bow_score test: 0.7512\n",
      "lr_tfidf_score test: 0.75\n",
      "lr_bow_score train: 0.996275\n",
      "lr_tfidf_score train: 0.996275\n"
     ]
    }
   ],
   "source": [
    "#Accuracy score for bag of words\n",
    "print(\"lr_bow_score test:\",evaluate(test_sentiments,lr_bow_predict)[0])\n",
    "#Accuracy score for tfidf features\n",
    "print(\"lr_tfidf_score test:\",evaluate(test_sentiments,lr_tfidf_predict)[0])\n",
    "#Accuracy score for bag of words\n",
    "print(\"lr_bow_score train:\",evaluate(train_sentiments,lr_bow_predict_train)[0])\n",
    "#Accuracy score for tfidf features\n",
    "print(\"lr_tfidf_score train:\",evaluate(train_sentiments,lr_tfidf_predict_train)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ac2ec8353acb5e0f548e1e4a590fbe6f34f4a686"
   },
   "source": [
    "**Print the classification report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "f89c7e7a6136d08790ffbf6bc4d0d05455f8555a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.75      0.75      0.75      4993\n",
      "    Positive       0.75      0.75      0.75      5007\n",
      "\n",
      "    accuracy                           0.75     10000\n",
      "   macro avg       0.75      0.75      0.75     10000\n",
      "weighted avg       0.75      0.75      0.75     10000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.74      0.77      0.75      4993\n",
      "    Positive       0.76      0.73      0.75      5007\n",
      "\n",
      "    accuracy                           0.75     10000\n",
      "   macro avg       0.75      0.75      0.75     10000\n",
      "weighted avg       0.75      0.75      0.75     10000\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Classification report for bag of words \n",
    "print(evaluate(test_sentiments,lr_bow_predict)[1])\n",
    "\n",
    "#Classification report for tfidf features\n",
    "print(print(evaluate(test_sentiments,lr_tfidf_predict)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8fde9753386e3593dc27c4e88e02bdc38462a018"
   },
   "source": [
    "**Stochastic gradient descent or Linear support vector machines for bag of words and tfidf features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "2211a9e97682195a0372b33e4da7267aad8548db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier(max_iter=2000, random_state=42)\n",
      "SGDClassifier(max_iter=2000, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "#training the linear svm\n",
    "svm=SGDClassifier(loss='hinge',max_iter=2000,random_state=42)\n",
    "#fitting the svm for bag of words\n",
    "svm_bow=svm.fit(cv_train_reviews,train_sentiments)\n",
    "print(svm_bow)\n",
    "#fitting the svm for tfidf features\n",
    "svm_tfidf=svm.fit(tv_train_reviews,train_sentiments)\n",
    "print(svm_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e9a7a973591c1d3cabaa1a47c57fa029d3752bab"
   },
   "source": [
    "**Model performance on test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "1a5ab738e04f0f9082c8d6ade6c2148cc398f8f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 ... 1 1 1]\n",
      "[1 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "#Predicting the model for bag of words\n",
    "svm_bow_predict=svm.predict(cv_test_reviews)\n",
    "print(svm_bow_predict)\n",
    "#Predicting the model for tfidf features\n",
    "svm_tfidf_predict=svm.predict(tv_test_reviews)\n",
    "print(svm_tfidf_predict)\n",
    "\n",
    "#Predicting the model for bag of words\n",
    "svm_bow_predict_train=svm.predict(cv_train_reviews)\n",
    "##Predicting the model for tfidf features\n",
    "svm_tfidf_predict_train=svm.predict(tv_train_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm_bow_score test: 0.5829\n",
      "svm_tfidf_score test: 0.5112\n",
      "svm_bow_score train: 0.990425\n",
      "svm_tfidf_score train: 0.990425\n"
     ]
    }
   ],
   "source": [
    "#Accuracy score for bag of words\n",
    "print(\"svm_bow_score test:\",evaluate(test_sentiments,svm_bow_predict)[0])\n",
    "#Accuracy score for tfidf features\n",
    "print(\"svm_tfidf_score test:\",evaluate(test_sentiments,svm_tfidf_predict)[0])\n",
    "\n",
    "#Accuracy score for bag of words\n",
    "print(\"svm_bow_score train:\",evaluate(train_sentiments,svm_bow_predict_train)[0])\n",
    "#Accuracy score for tfidf features\n",
    "print(\"svm_tfidf_score train:\",evaluate(train_sentiments,svm_tfidf_predict_train)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b1bd245f50902ad87ca28e48cbce64ec6a16ec5a"
   },
   "source": [
    "**Print the classification report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "d112bc5b4944330b567e19a7e04544a9a459f238"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.94      0.18      0.30      4993\n",
      "    Positive       0.55      0.99      0.70      5007\n",
      "\n",
      "    accuracy                           0.58     10000\n",
      "   macro avg       0.74      0.58      0.50     10000\n",
      "weighted avg       0.74      0.58      0.50     10000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      0.02      0.04      4993\n",
      "    Positive       0.51      1.00      0.67      5007\n",
      "\n",
      "    accuracy                           0.51     10000\n",
      "   macro avg       0.75      0.51      0.36     10000\n",
      "weighted avg       0.75      0.51      0.36     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification report for bag of words \n",
    "print(evaluate(test_sentiments,svm_bow_predict)[1])\n",
    "#Classification report for tfidf features\n",
    "print(evaluate(test_sentiments,svm_tfidf_predict)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multinomial Naive Bayes for bag of words and tfidf features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB()\n",
      "MultinomialNB()\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "mnb=MultinomialNB()\n",
    "#fitting the svm for bag of words\n",
    "mnb_bow=mnb.fit(cv_train_reviews,train_sentiments)\n",
    "print(mnb_bow)\n",
    "#fitting the svm for tfidf features\n",
    "mnb_tfidf=mnb.fit(tv_train_reviews,train_sentiments)\n",
    "print(mnb_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model performance on test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 1 1]\n",
      "[0 0 0 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "#Predicting the model for bag of words\n",
    "mnb_bow_predict=mnb.predict(cv_test_reviews)\n",
    "print(mnb_bow_predict)\n",
    "#Predicting the model for tfidf features\n",
    "mnb_tfidf_predict=mnb.predict(tv_test_reviews)\n",
    "print(mnb_tfidf_predict)\n",
    "\n",
    "#Predicting the model for bag of words\n",
    "mnb_bow_predict_train=mnb.predict(cv_train_reviews)\n",
    "##Predicting the model for tfidf features\n",
    "mnb_tfidf_predict_train=mnb.predict(tv_train_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnb_bow_score test: 0.751\n",
      "mnb_bow_score train: 0.996275\n",
      "mnb_tfidf_score test: 0.7509\n",
      "mnb_tfidf_score train: 0.996275\n"
     ]
    }
   ],
   "source": [
    "#Accuracy score for bag of words\n",
    "print(\"mnb_bow_score test:\",evaluate(test_sentiments,mnb_bow_predict)[0])\n",
    "print(\"mnb_bow_score train:\",evaluate(train_sentiments,mnb_bow_predict_train)[0])\n",
    "#Accuracy score for tfidf features\n",
    "print(\"mnb_tfidf_score test:\",evaluate(test_sentiments,mnb_tfidf_predict)[0])\n",
    "print(\"mnb_tfidf_score train:\",evaluate(train_sentiments,mnb_tfidf_predict_train)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print the classification report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification report for bag of words \n",
    "print(evaluate(test_sentiments,mnb_bow_predict)[1])\n",
    "#Classification report for tfidf features\n",
    "print(evaluate(test_sentiments,mnb_tfidf_predict)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c1eac46bfc9221687887cfb6dac019e54e452a3a6e2b82ef73adb9c089464026"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
