{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "**Sentiment Analysis of IMDB Movie Reviews**\n",
    "\n",
    "\n",
    "This Notebook is heavily based on the Notebook by [Lakshmipathi N](https://www.kaggle.com/lakshmi25npathi) found on [Kaggle](https://www.kaggle.com/lakshmi25npathi/sentiment-analysis-of-imdb-movie-reviews)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1424638f5259100af9f9a5c1b05bd23cf5b71e51"
   },
   "source": [
    "**Import necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#Load the libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "# https://online.stat.psu.edu/stat504/lesson/1/1.7\n",
    "from utils import preprocesser_text, binarize_sentiment, train_test_split, evaluate\n",
    "\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "be1b642cce343f7a8f68f8c91f7c50372cdf4381"
   },
   "source": [
    "**Import the training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "4c593c17588723c0b0b0f19851cb70a8447ced76",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing the training data\n",
    "imdb_data=pd.read_csv('data/IMDB Dataset.csv')\n",
    "print(imdb_data.shape)\n",
    "imdb_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1ad3773974351ed9bdf389b2847d7475b36c2295"
   },
   "source": [
    "**Exploratery data analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "7f11c83b1320c8982b36889145f7f770563674a8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>49582</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review sentiment\n",
       "count                                               50000     50000\n",
       "unique                                              49582         2\n",
       "top     Loved today's show!!! It was a variety and not...  positive\n",
       "freq                                                    5     25000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Summary of the dataset\n",
    "imdb_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "453c3fd238f62ab8f649eb01771817e25bc0c77d"
   },
   "source": [
    "**Sentiment count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "cb6bb97b0f851947dcf341a1de5708a1f2bc64c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    25000\n",
       "negative    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentiment count\n",
    "imdb_data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dataset is balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f61964573faababe1f7897b77d32815a24954d2f"
   },
   "source": [
    "**Spliting the training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 50000/50000 [00:09<00:00, 5118.26it/s]\n",
      "Pandas Apply: 100%|██████████| 50000/50000 [00:00<00:00, 274732.95it/s]\n",
      "Pandas Apply: 100%|██████████| 50000/50000 [00:01<00:00, 29940.49it/s]\n",
      "Pandas Apply: 100%|██████████| 50000/50000 [03:19<00:00, 250.78it/s]\n",
      "Pandas Apply: 100%|██████████| 50000/50000 [00:49<00:00, 1017.21it/s]\n"
     ]
    }
   ],
   "source": [
    "imdb_data_norm = preprocesser_text(imdb_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "90da29c3b79f46f41d7391a2a116065b616d0fac"
   },
   "source": [
    "**Text normalization**\n",
    "When doing a very basic train/test-split, we should be sure that we have close to a 50/50 Balance of Classes in the Train-Test set. This is the case here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "b20c242bd091929ca896ea2c6e936ca00efe6ecf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative    20007\n",
      "positive    19993\n",
      "Name: sentiment, dtype: int64\n",
      "positive    5007\n",
      "negative    4993\n",
      "Name: sentiment, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'one review ha mention watch 1 oz episod youll hook right thi exactli happen meth first thing struck oz wa brutal unflinch scene violenc set right word go trust thi show faint heart timid thi show pull punch regard drug sex violenc hardcor classic use wordit call oz nicknam given oswald maximum secur state penitentari focus mainli emerald citi experiment section prison cell glass front face inward privaci high agenda em citi home manyaryan muslim gangsta latino christian italian irish moreso scuffl death stare dodgi deal shadi agreement never far awayi would say main appeal show due fact goe show wouldnt dare forget pretti pictur paint mainstream audienc forget charm forget romanceoz doesnt mess around first episod ever saw struck nasti wa surreal couldnt say wa readi watch develop tast oz got accustom high level graphic violenc violenc injustic crook guard wholl sold nickel inmat wholl kill order get away well manner middl class inmat turn prison bitch due lack street skill prison experi watch oz may becom comfort uncomfort viewingthat get touch darker side'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalized train reviews\n",
    "norm_train, norm_test = train_test_split(imdb_data_norm)\n",
    "print(norm_train.sentiment.value_counts())\n",
    "print(norm_test.sentiment.value_counts())\n",
    "norm_train.review[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "52371868f05ff9cf157280c5acf0f5bc71ee176d"
   },
   "source": [
    "**Bags of words model**\n",
    "\n",
    "It is used to convert text documents to numerical vectors. It also creates n_grams, which basically means that if n = one, one word == 1 vector. If n = 2, a vector is made up of two words etc. One row then is equal to how often a specific n_gram appears in the, in this case, Review.\n",
    "min_df: float x: Means that a word has to appear in at least x% of documents\n",
    "max_df: float x: Means that a word has to appear in maximum x% of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW_cv_train: (40000, 17053)\n",
      "BOW_cv_test: (10000, 17053)\n",
      "Vocab:  ['010', '10', '10 10', '10 becaus', '10 line']\n",
      "Vocab Length:  17053\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1x17053 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 143 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count vectorizer for bag of words\n",
    "cv=CountVectorizer(ngram_range=(1,3), min_df=0.001, max_df=0.999)\n",
    "#transformed train reviews\n",
    "cv_train_reviews=cv.fit_transform(norm_train.review)\n",
    "#transformed test reviews\n",
    "cv_test_reviews=cv.transform(norm_test.review)\n",
    "\n",
    "print('BOW_cv_train:',cv_train_reviews.shape)\n",
    "print('BOW_cv_test:',cv_test_reviews.shape)\n",
    "print('Vocab: ', cv.get_feature_names()[:5])\n",
    "print('Vocab Length: ', len(cv.get_feature_names()))\n",
    "cv_train_reviews[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "52371868f05ff9cf157280c5acf0f5bc71ee176d"
   },
   "source": [
    "**Term Frequency-Inverse Document Frequency model (TFIDF)**\n",
    "\n",
    "It is used to convert text documents to  matrix of  tfidf features. Basically, per review it will calculate how often a word appears in this review and multiples it with the inverse document frequency, which is basically a \"punishing\" value if a word appears in many documents. Basically: # number of documents / # documents containing the word. \n",
    "\n",
    "> The formula that is used to compute the tf-idf for a term t of a document d in a document set is tf-idf(t, d) = tf(t, d) * idf(t), and the idf is computed as idf(t) = log [ n / df(t) ] + 1 (if smooth_idf=False), where n is the total number of documents in the document set and df(t) is the document frequency of t; the document frequency is the number of documents in the document set that contain the term t. The effect of adding “1” to the idf in the equation above is that terms with zero idf, i.e., terms that occur in all documents in a training set, will not be entirely ignored. (Note that the idf formula above differs from the standard textbook notation that defines the idf as idf(t) = log [ n / (df(t) + 1) ]).\n",
    "\n",
    "tf(t, d) == # Wort t / max (# Wort t) über alle Dokumente\n",
    "\n",
    "idf(t) == # Dokumente mit Wort t / # Dokumente\n",
    "\n",
    "\n",
    "tf-idf(t, d) = tf(t, d) * idf(t)\n",
    "\n",
    "min_df: float x: Means that a word has to appear in at least x% of documents\n",
    "max_df: float x: Means that a word has to appear in maximum x% of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf_train: (40000, 6983231)\n",
      "Tfidf_test: (10000, 6983231)\n",
      "Tfidf_test: 6983231\n"
     ]
    }
   ],
   "source": [
    "#Tfidf vectorizer\n",
    "tv=TfidfVectorizer(ngram_range=(1,3), min_df=0., max_df=1.)\n",
    "#transformed train reviews\n",
    "tv_train_reviews=tv.fit_transform(norm_train.review)\n",
    "#transformed test reviews\n",
    "tv_test_reviews=tv.transform(norm_test.review)\n",
    "print('Tfidf_train:',tv_train_reviews.shape)\n",
    "print('Tfidf_test:',tv_test_reviews.shape)\n",
    "print('Tfidf_test:',len(tv.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "afe6de957339921e05a6faeaf731f2272fd31946"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf_train: (40000, 17053)\n",
      "Tfidf_test: (10000, 17053)\n",
      "Tfidf_test: 17053\n"
     ]
    }
   ],
   "source": [
    "#Tfidf vectorizer\n",
    "tv=TfidfVectorizer(ngram_range=(1,3), min_df=0.001, max_df=0.999)\n",
    "#transformed train reviews\n",
    "tv_train_reviews=tv.fit_transform(norm_train.review)\n",
    "#transformed test reviews\n",
    "tv_test_reviews=tv.transform(norm_test.review)\n",
    "print('Tfidf_train:',tv_train_reviews.shape)\n",
    "print('Tfidf_test:',tv_test_reviews.shape)\n",
    "print('Tfidf_test:',len(tv.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "21a80c94fb42e14391c627710c5d796c40aa7dde"
   },
   "source": [
    "**Split and binarize the sentiment tdata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "ca1e4cc917265ac98a72c37cffe57f27e9897408"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "#Spliting the sentiment data\n",
    "train_sentiments = binarize_sentiment(norm_train.sentiment)\n",
    "test_sentiments = binarize_sentiment(norm_test.sentiment)\n",
    "print(train_sentiments.unique())\n",
    "print(test_sentiments.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelling the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d5e45fdc9d062a5b9b9dd665ffe732776e196953"
   },
   "source": [
    "To compare different parameters for the model and text preprocesser, we can create a function, which does all the preprocessing and modelling. This allows us to quickly compare and evaluate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_train_validate_evaluate(word_vectorizer, n_gram, analyzer, min_df, max_df, model, parameters, train_x, train_y, test_x, test_y):\n",
    "    \"\"\"Pipeline to compare different types of preprocessing of words, models and parameters.\"\"\"\n",
    "    word_vectorizer = word_vectorizer(ngram_range=n_gram, analyzer=analyzer, min_df=min_df, max_df=max_df)\n",
    "    train_x = word_vectorizer.fit_transform(train_x)\n",
    "    test_x = word_vectorizer.transform(test_x)\n",
    "    print('Length of Vocabulary after vectorizing the corpus:',len(word_vectorizer.vocabulary_))\n",
    "    # Gridsearch to find the best values:\n",
    "    grid_search = GridSearchCV(estimator = model,\n",
    "                            param_grid = parameters, \n",
    "                            scoring = 'accuracy',\n",
    "                            cv = 5,\n",
    "                            n_jobs = -1, \n",
    "                            verbose = 2)\n",
    "    grid_search.fit(train_x,train_y)\n",
    "    print(f'Best Score: {grid_search.best_score_}. Best Params: {grid_search.best_params_}')\n",
    "\n",
    "    #Fitting the model for tfidf features\n",
    "    grid_search.best_estimator_.fit(train_x,train_y)\n",
    "    print(grid_search.best_estimator_)\n",
    "\n",
    "    #Predicting test\n",
    "    y_pred=grid_search.best_estimator_.predict(test_x)\n",
    "    print(y_pred)\n",
    "\n",
    "    #Predicting train\n",
    "    y_pred_train=grid_search.best_estimator_.predict(train_x)\n",
    "    print(y_pred_train)\n",
    "    ##Predicting the model for tfidf features\n",
    "\n",
    "    #Accuracy score test\n",
    "    print(\"Accuracy test:\",evaluate(test_y,y_pred)[0])\n",
    "    #Accuracy score train\n",
    "    print(\"Accuracy train:\",evaluate(train_y,y_pred_train)[0])\n",
    "    # Confiusionmatrix\n",
    "    print(\"Accuracy test:\",evaluate(test_y,y_pred)[1])\n",
    "    #Accuracy score train\n",
    "    print(\"Accuracy train:\",evaluate(train_y,y_pred_train)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "_uuid": "142d007421900550079a12ae8655bcae678ebaad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Vocabulary after vectorizing the corpus: 17053\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best Score: 0.8937250000000001. Best Params: {'C': 1, 'max_iter': 500, 'penalty': 'l2'}\n",
      "LogisticRegression(C=1, max_iter=500)\n",
      "[0 0 0 ... 1 0 0]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.8986\n",
      "Accuracy train: 0.929425\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.90      0.90      0.90      4993\n",
      "    Positive       0.90      0.90      0.90      5007\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.94      0.92      0.93     20007\n",
      "    Positive       0.92      0.94      0.93     19993\n",
      "\n",
      "    accuracy                           0.93     40000\n",
      "   macro avg       0.93      0.93      0.93     40000\n",
      "weighted avg       0.93      0.93      0.93     40000\n",
      "\n",
      "Length of Vocabulary after vectorizing the corpus: 17053\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best Score: 0.8912000000000001. Best Params: {'C': 0.1, 'max_iter': 500, 'penalty': 'l2'}\n",
      "LogisticRegression(C=0.1, max_iter=500)\n",
      "[0 0 1 ... 0 0 0]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.8925\n",
      "Accuracy train: 0.964575\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.89      0.89      0.89      4993\n",
      "    Positive       0.89      0.89      0.89      5007\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.97      0.96      0.96     20007\n",
      "    Positive       0.96      0.97      0.96     19993\n",
      "\n",
      "    accuracy                           0.96     40000\n",
      "   macro avg       0.96      0.96      0.96     40000\n",
      "weighted avg       0.96      0.96      0.96     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "lr=LogisticRegression()\n",
    "tv=TfidfVectorizer\n",
    "cv=CountVectorizer\n",
    "# Gridsearch to find the best values:\n",
    "parameters = [{'C': [0.01,0.1,1], 'penalty': ['l2'], 'max_iter':[500]}]\n",
    "# Count vectorizer\n",
    "vectorize_train_validate_evaluate(tv, n_gram=(1,3), analyzer='word', min_df=0.001, max_df=0.999, \n",
    "                                    model=lr, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)\n",
    "\n",
    "# Tfidf\n",
    "vectorize_train_validate_evaluate(cv, n_gram=(1,3), analyzer='word', min_df=0.001, max_df=0.999, \n",
    "                                    model=lr, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chaning the ngram-range**\n",
    "\n",
    "As seen below, the training speeds up quickly when chaning the range of ngrams to 3. However, the accuracy on the test-set also drops from 89% to 67%, which is below our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Vocabulary after vectorizing the corpus: 789\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best Score: 0.653425. Best Params: {'C': 1, 'max_iter': 500, 'penalty': 'l2'}\n",
      "LogisticRegression(C=1, max_iter=500)\n",
      "[1 0 0 ... 1 0 0]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.6569\n",
      "Accuracy train: 0.670275\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.73      0.50      0.59      4993\n",
      "    Positive       0.62      0.81      0.70      5007\n",
      "\n",
      "    accuracy                           0.66     10000\n",
      "   macro avg       0.67      0.66      0.65     10000\n",
      "weighted avg       0.67      0.66      0.65     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.75      0.52      0.61     20007\n",
      "    Positive       0.63      0.82      0.71     19993\n",
      "\n",
      "    accuracy                           0.67     40000\n",
      "   macro avg       0.69      0.67      0.66     40000\n",
      "weighted avg       0.69      0.67      0.66     40000\n",
      "\n",
      "Length of Vocabulary after vectorizing the corpus: 789\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best Score: 0.65125. Best Params: {'C': 1, 'max_iter': 500, 'penalty': 'l2'}\n",
      "LogisticRegression(C=1, max_iter=500)\n",
      "[1 0 0 ... 1 0 0]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.6552\n",
      "Accuracy train: 0.669675\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.73      0.49      0.59      4993\n",
      "    Positive       0.62      0.82      0.70      5007\n",
      "\n",
      "    accuracy                           0.66     10000\n",
      "   macro avg       0.67      0.65      0.65     10000\n",
      "weighted avg       0.67      0.66      0.65     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.76      0.50      0.60     20007\n",
      "    Positive       0.63      0.84      0.72     19993\n",
      "\n",
      "    accuracy                           0.67     40000\n",
      "   macro avg       0.69      0.67      0.66     40000\n",
      "weighted avg       0.69      0.67      0.66     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "lr=LogisticRegression()\n",
    "tv=TfidfVectorizer\n",
    "cv=CountVectorizer\n",
    "# Gridsearch to find the best values:\n",
    "parameters = [{'C': [0.01,0.1,1], 'penalty': ['l2'], 'max_iter':[500]}]\n",
    "# Count vectorizer\n",
    "vectorize_train_validate_evaluate(tv, n_gram=(3,3), analyzer='word', min_df=0.001, max_df=0.999, \n",
    "                                    model=lr, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)\n",
    "\n",
    "# Tfidf\n",
    "vectorize_train_validate_evaluate(cv, n_gram=(3,3), analyzer='word', min_df=0.001, max_df=0.999, \n",
    "                                    model=lr, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using chars as ngram**\n",
    "\n",
    "We can see that the vocabulary-size drops from 17'000 to 7'900, even though we have the same n_gram range. The Quality on the test-set also drops by 2.7%. \n",
    "\n",
    "**Linear Support Vector Classifier**\n",
    "\n",
    "The Hyperparameter-tuning and Training is very quick. Also, the Quality on the Test-Set on both bow and tfidf is similar with around 89.9%. Thus, we will try differnet compbinations of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Vocabulary after vectorizing the corpus: 17053\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best Score: 0.894725. Best Params: {'C': 0.1, 'max_iter': 500, 'penalty': 'l2'}\n",
      "LinearSVC(C=0.1, max_iter=500)\n",
      "[0 0 1 ... 1 0 0]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.899\n",
      "Accuracy train: 0.931325\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.90      0.89      0.90      4993\n",
      "    Positive       0.90      0.90      0.90      5007\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.94      0.92      0.93     20007\n",
      "    Positive       0.92      0.94      0.93     19993\n",
      "\n",
      "    accuracy                           0.93     40000\n",
      "   macro avg       0.93      0.93      0.93     40000\n",
      "weighted avg       0.93      0.93      0.93     40000\n",
      "\n",
      "Length of Vocabulary after vectorizing the corpus: 17053\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best Score: 0.8914500000000001. Best Params: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2'}\n",
      "LinearSVC(C=0.01, max_iter=500)\n",
      "[0 0 1 ... 0 0 0]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.8938\n",
      "Accuracy train: 0.967125\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.90      0.89      0.89      4993\n",
      "    Positive       0.89      0.90      0.89      5007\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.97      0.96      0.97     20007\n",
      "    Positive       0.96      0.97      0.97     19993\n",
      "\n",
      "    accuracy                           0.97     40000\n",
      "   macro avg       0.97      0.97      0.97     40000\n",
      "weighted avg       0.97      0.97      0.97     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "svc=LinearSVC()\n",
    "tv=TfidfVectorizer\n",
    "cv=CountVectorizer\n",
    "# Gridsearch to find the best values:\n",
    "parameters = [{'C': [0.01,0.1,1], 'penalty': ['l2'], 'max_iter':[500]}]\n",
    "# Count vectorizer\n",
    "vectorize_train_validate_evaluate(tv, n_gram=(1,3), analyzer='word', min_df=0.001, max_df=0.999, \n",
    "                                    model=svc, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)\n",
    "\n",
    "# Tfidf\n",
    "vectorize_train_validate_evaluate(cv, n_gram=(1,3), analyzer='word', min_df=0.001, max_df=0.999, \n",
    "                                    model=svc, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Vocabulary after vectorizing the corpus: 153\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best Score: 0.7661250000000001. Best Params: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2'}\n",
      "LinearSVC(C=0.01, max_iter=500)\n",
      "[0 0 0 ... 1 0 0]\n",
      "[1 1 1 ... 1 1 0]\n",
      "Accuracy test: 0.7632\n",
      "Accuracy train: 0.767975\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.77      0.75      0.76      4993\n",
      "    Positive       0.76      0.77      0.77      5007\n",
      "\n",
      "    accuracy                           0.76     10000\n",
      "   macro avg       0.76      0.76      0.76     10000\n",
      "weighted avg       0.76      0.76      0.76     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.77      0.76      0.77     20007\n",
      "    Positive       0.76      0.77      0.77     19993\n",
      "\n",
      "    accuracy                           0.77     40000\n",
      "   macro avg       0.77      0.77      0.77     40000\n",
      "weighted avg       0.77      0.77      0.77     40000\n",
      "\n",
      "Length of Vocabulary after vectorizing the corpus: 153\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best Score: 0.7645. Best Params: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2'}\n",
      "LinearSVC(C=0.01, max_iter=500)\n",
      "[0 0 0 ... 1 0 0]\n",
      "[1 1 1 ... 1 1 0]\n",
      "Accuracy test: 0.7625\n",
      "Accuracy train: 0.7678\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.77      0.75      0.76      4993\n",
      "    Positive       0.76      0.78      0.77      5007\n",
      "\n",
      "    accuracy                           0.76     10000\n",
      "   macro avg       0.76      0.76      0.76     10000\n",
      "weighted avg       0.76      0.76      0.76     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.78      0.75      0.76     20007\n",
      "    Positive       0.76      0.78      0.77     19993\n",
      "\n",
      "    accuracy                           0.77     40000\n",
      "   macro avg       0.77      0.77      0.77     40000\n",
      "weighted avg       0.77      0.77      0.77     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "svc=LinearSVC()\n",
    "tv=TfidfVectorizer\n",
    "cv=CountVectorizer\n",
    "# Gridsearch to find the best values:\n",
    "parameters = [{'C': [0.01,0.1,1], 'penalty': ['l2'], 'max_iter':[500]}]\n",
    "# Count vectorizer\n",
    "vectorize_train_validate_evaluate(tv, n_gram=(1,3), analyzer='word', min_df=0.1, max_df=0.9, \n",
    "                                    model=svc, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)\n",
    "\n",
    "# Tfidf\n",
    "vectorize_train_validate_evaluate(cv, n_gram=(1,3), analyzer='word', min_df=0.1, max_df=0.9, \n",
    "                                    model=svc, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that if we drop words, which appear in less than 10% of documents and those, which in more than 90% documents, we have a vocabulary size of 153. This leads to an awful test-accuracy of 76.6%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Vocabulary after vectorizing the corpus: 17108\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best Score: 0.89495. Best Params: {'C': 0.1, 'max_iter': 500, 'penalty': 'l2'}\n",
      "LinearSVC(C=0.1, max_iter=500)\n",
      "[0 0 1 ... 1 0 0]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.8991\n",
      "Accuracy train: 0.93145\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.90      0.89      0.90      4993\n",
      "    Positive       0.90      0.90      0.90      5007\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.94      0.92      0.93     20007\n",
      "    Positive       0.92      0.94      0.93     19993\n",
      "\n",
      "    accuracy                           0.93     40000\n",
      "   macro avg       0.93      0.93      0.93     40000\n",
      "weighted avg       0.93      0.93      0.93     40000\n",
      "\n",
      "Length of Vocabulary after vectorizing the corpus: 17108\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best Score: 0.89155. Best Params: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2'}\n",
      "LinearSVC(C=0.01, max_iter=500)\n",
      "[0 0 1 ... 0 0 0]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.8936\n",
      "Accuracy train: 0.966975\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.90      0.89      0.89      4993\n",
      "    Positive       0.89      0.90      0.89      5007\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.97      0.96      0.97     20007\n",
      "    Positive       0.96      0.97      0.97     19993\n",
      "\n",
      "    accuracy                           0.97     40000\n",
      "   macro avg       0.97      0.97      0.97     40000\n",
      "weighted avg       0.97      0.97      0.97     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "svc=LinearSVC()\n",
    "tv=TfidfVectorizer\n",
    "cv=CountVectorizer\n",
    "# Gridsearch to find the best values:\n",
    "parameters = [{'C': [0.01,0.1,1], 'penalty': ['l2'], 'max_iter':[500]}]\n",
    "# Count vectorizer\n",
    "vectorize_train_validate_evaluate(tv, n_gram=(1,4), analyzer='word', min_df=0.001, max_df=0.999, \n",
    "                                    model=svc, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)\n",
    "\n",
    "# Tfidf\n",
    "vectorize_train_validate_evaluate(cv, n_gram=(1,4), analyzer='word', min_df=0.001, max_df=0.999, \n",
    "                                    model=svc, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the ngram-range for the word-vectorizer increases the test-accuracy slightly to 89.91% and the length of the vocabulary to 17108."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Vocabulary after vectorizing the corpus: 7890\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best Score: 0.8679. Best Params: {'C': 1, 'max_iter': 500, 'penalty': 'l2'}\n",
      "LinearSVC(C=1, max_iter=500)\n",
      "[0 0 1 ... 1 1 0]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.8732\n",
      "Accuracy train: 0.9112\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.88      0.87      0.87      4993\n",
      "    Positive       0.87      0.88      0.87      5007\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.92      0.91      0.91     20007\n",
      "    Positive       0.91      0.92      0.91     19993\n",
      "\n",
      "    accuracy                           0.91     40000\n",
      "   macro avg       0.91      0.91      0.91     40000\n",
      "weighted avg       0.91      0.91      0.91     40000\n",
      "\n",
      "Length of Vocabulary after vectorizing the corpus: 7890\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vince\\anaconda3\\envs\\npr\\lib\\site-packages\\sklearn\\svm\\_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.8568. Best Params: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vince\\anaconda3\\envs\\npr\\lib\\site-packages\\sklearn\\svm\\_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=0.01, max_iter=500)\n",
      "[0 0 1 ... 1 1 0]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.8647\n",
      "Accuracy train: 0.922575\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.86      0.87      0.87      4993\n",
      "    Positive       0.87      0.86      0.86      5007\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.92      0.92      0.92     20007\n",
      "    Positive       0.92      0.92      0.92     19993\n",
      "\n",
      "    accuracy                           0.92     40000\n",
      "   macro avg       0.92      0.92      0.92     40000\n",
      "weighted avg       0.92      0.92      0.92     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "svc=LinearSVC()\n",
    "tv=TfidfVectorizer\n",
    "cv=CountVectorizer\n",
    "# Gridsearch to find the best values:\n",
    "parameters = [{'C': [0.01,0.1,1], 'penalty': ['l2'], 'max_iter':[500]}]\n",
    "# Count vectorizer\n",
    "vectorize_train_validate_evaluate(tv, n_gram=(1,3), analyzer='char', min_df=0.001, max_df=0.999, \n",
    "                                    model=svc, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)\n",
    "\n",
    "# Tfidf\n",
    "vectorize_train_validate_evaluate(cv, n_gram=(1,3), analyzer='char', min_df=0.001, max_df=0.999, \n",
    "                                    model=svc, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a char as analyzer, we get a vocabulary of 7890. However, this does not decrease the training-time though, because it does not converge in 500 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multinomial Naive Bayes for bag of words and tfidf features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Vocabulary after vectorizing the corpus: 17053\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best Score: 0.872475. Best Params: {'alpha': 0.01}\n",
      "MultinomialNB(alpha=0.01)\n",
      "[0 0 0 ... 0 0 0]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.8739\n",
      "Accuracy train: 0.891575\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.88      0.86      0.87      4993\n",
      "    Positive       0.87      0.89      0.88      5007\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.91      0.87      0.89     20007\n",
      "    Positive       0.88      0.91      0.89     19993\n",
      "\n",
      "    accuracy                           0.89     40000\n",
      "   macro avg       0.89      0.89      0.89     40000\n",
      "weighted avg       0.89      0.89      0.89     40000\n",
      "\n",
      "Length of Vocabulary after vectorizing the corpus: 17053\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best Score: 0.861375. Best Params: {'alpha': 0.01}\n",
      "MultinomialNB(alpha=0.01)\n",
      "[0 0 0 ... 0 1 0]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.8642\n",
      "Accuracy train: 0.8757\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.87      0.86      0.86      4993\n",
      "    Positive       0.86      0.87      0.86      5007\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.88      0.87      0.87     20007\n",
      "    Positive       0.87      0.88      0.88     19993\n",
      "\n",
      "    accuracy                           0.88     40000\n",
      "   macro avg       0.88      0.88      0.88     40000\n",
      "weighted avg       0.88      0.88      0.88     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "mnb=MultinomialNB()\n",
    "tv=TfidfVectorizer\n",
    "cv=CountVectorizer\n",
    "# Gridsearch to find the best values:\n",
    "parameters = [{'alpha': [0.01,0.1,1]}]\n",
    "# Count vectorizer\n",
    "vectorize_train_validate_evaluate(tv, n_gram=(1,3), analyzer='word', min_df=0.001, max_df=0.999, \n",
    "                                    model=mnb, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)\n",
    "\n",
    "# Tfidf\n",
    "vectorize_train_validate_evaluate(cv, n_gram=(1,3), analyzer='word', min_df=0.001, max_df=0.999, \n",
    "                                    model=mnb, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Vocabulary after vectorizing the corpus: 7890\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best Score: 0.815475. Best Params: {'alpha': 1}\n",
      "MultinomialNB(alpha=1)\n",
      "[0 0 0 ... 1 1 0]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.8142\n",
      "Accuracy train: 0.82485\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.82      0.80      0.81      4993\n",
      "    Positive       0.81      0.83      0.82      5007\n",
      "\n",
      "    accuracy                           0.81     10000\n",
      "   macro avg       0.81      0.81      0.81     10000\n",
      "weighted avg       0.81      0.81      0.81     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.83      0.81      0.82     20007\n",
      "    Positive       0.82      0.84      0.83     19993\n",
      "\n",
      "    accuracy                           0.82     40000\n",
      "   macro avg       0.83      0.82      0.82     40000\n",
      "weighted avg       0.83      0.82      0.82     40000\n",
      "\n",
      "Length of Vocabulary after vectorizing the corpus: 7890\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best Score: 0.7774749999999999. Best Params: {'alpha': 0.01}\n",
      "MultinomialNB(alpha=0.01)\n",
      "[0 0 0 ... 1 1 0]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.7775\n",
      "Accuracy train: 0.783825\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.77      0.79      0.78      4993\n",
      "    Positive       0.79      0.76      0.77      5007\n",
      "\n",
      "    accuracy                           0.78     10000\n",
      "   macro avg       0.78      0.78      0.78     10000\n",
      "weighted avg       0.78      0.78      0.78     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.77      0.80      0.79     20007\n",
      "    Positive       0.79      0.77      0.78     19993\n",
      "\n",
      "    accuracy                           0.78     40000\n",
      "   macro avg       0.78      0.78      0.78     40000\n",
      "weighted avg       0.78      0.78      0.78     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "mnb=MultinomialNB()\n",
    "tv=TfidfVectorizer\n",
    "cv=CountVectorizer\n",
    "# Gridsearch to find the best values:\n",
    "parameters = [{'alpha': [0.01,0.1,1]}]\n",
    "# Count vectorizer\n",
    "vectorize_train_validate_evaluate(tv, n_gram=(1,3), analyzer='char', min_df=0.001, max_df=0.999, \n",
    "                                    model=mnb, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)\n",
    "\n",
    "# Tfidf\n",
    "vectorize_train_validate_evaluate(cv, n_gram=(1,3), analyzer='char', min_df=0.001, max_df=0.999, \n",
    "                                    model=mnb, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "407318ab8a71c1edb50740ee323cc7307e05568546678b79c25c054004b7234a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
